{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp synchro.extracting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# synchro.extracting\n",
    "> Function to extract data of an experiment from 3rd party programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To align timeseries of an experiment, we need to read logs and import data produced by 3rd party softwares used during the experiment. It includes:\n",
    "* QDSpy logging\n",
    "* Numpy arrays of the stimuli\n",
    "* SpykingCircus spike sorting refined with Phy\n",
    "* Eye tracking results from MaskRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os, glob\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from theonerig.synchro.io import *\n",
    "from theonerig.utils import *\n",
    "\n",
    "def get_QDSpy_logs(log_dir):\n",
    "    \"\"\"Factory function to generate QDSpy_log objects from all the QDSpy logs of the folder `log_dir`\"\"\"\n",
    "    log_names = glob.glob(os.path.join(log_dir,'[0-9]*.log'))\n",
    "    qdspy_logs = [QDSpy_log(log_name) for log_name in log_names]\n",
    "    for qdspy_log in qdspy_logs:\n",
    "        qdspy_log.find_stimuli()\n",
    "    return qdspy_logs\n",
    "\n",
    "class QDSpy_log:\n",
    "    \"\"\"Class defining a QDSpy log. \n",
    "    It reads the log it represent and extract the stimuli information from it:\n",
    "      - Start and end time\n",
    "      - Parameters like the md5 key\n",
    "      - Frame delays\n",
    "    \"\"\"\n",
    "    def __init__(self, log_path):\n",
    "        self.log_path = log_path\n",
    "        self.stimuli = []\n",
    "        self.comments = []\n",
    "        \n",
    "    def _extract_data(self, data_line):\n",
    "        data = data_line[data_line.find('{')+1:data_line.find('}')]\n",
    "        data_splitted = data.split(',')\n",
    "        data_dict = {}\n",
    "        for data in data_splitted:\n",
    "            ind = data.find(\"'\")\n",
    "            if type(data[data.find(\":\")+2:]) is str:\n",
    "                data_dict[data[ind+1:data.find(\"'\",ind+1)]] = data[data.find(\":\")+2:][1:-1]\n",
    "            else:\n",
    "                data_dict[data[ind+1:data.find(\"'\",ind+1)]] = data[data.find(\":\")+2:]\n",
    "        return data_dict\n",
    "\n",
    "    def _extract_time(self,data_line):\n",
    "        return datetime.datetime.strptime(data_line.split()[0], '%Y%m%d_%H%M%S')\n",
    "    \n",
    "    def _extract_delay(self,data_line):\n",
    "        ind = data_line.find('#')\n",
    "        index_frame = int(data_line[ind+1:data_line.find(' ',ind)])\n",
    "        ind = data_line.find('was')\n",
    "        delay = float(data_line[ind:].split(\" \")[1])\n",
    "        return (index_frame, delay)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"\\n\".join([str(stim) for stim in self.stimuli])\n",
    "    \n",
    "    @property\n",
    "    def n_stim(self):\n",
    "        return len(self.stimuli)\n",
    "\n",
    "    @property\n",
    "    def stim_names(self):\n",
    "        return [stim.name for stim in self.stimuli]\n",
    "    \n",
    "    def find_stimuli(self):\n",
    "        \"\"\"Find the stimuli in the log file and return the list of the stimuli\n",
    "        found by this object.\"\"\"\n",
    "        with open(self.log_path, 'r', encoding=\"ISO-8859-1\") as log_file:\n",
    "            for line in log_file:\n",
    "                if \"DATA\" in line:\n",
    "                    data_juice = self._extract_data(line)\n",
    "                    if 'stimState' in data_juice.keys():\n",
    "                        if data_juice['stimState'] == \"STARTED\" :\n",
    "                            curr_stim = Stimulus(self._extract_time(line))\n",
    "                            curr_stim.set_parameters(data_juice)\n",
    "                            self.stimuli.append(curr_stim)\n",
    "                            stimulus_ON = True\n",
    "                        elif data_juice['stimState'] == \"FINISHED\" or data_juice['stimState'] == \"ABORTED\":\n",
    "                            curr_stim.is_aborted = data_juice['stimState'] == \"ABORTED\"\n",
    "                            curr_stim.stop_time = self._extract_time(line)\n",
    "                            stimulus_ON = False\n",
    "\n",
    "                    elif 'userComment' in data_juice.keys():\n",
    "                        pass\n",
    "                        #print(\"userComment, use it to bind logs to records\")\n",
    "                    elif stimulus_ON: #Information on stimulus parameters\n",
    "                        curr_stim.set_parameters(data_juice)\n",
    "    #                elif 'probeX' in data_juice.keys():\n",
    "            #            print(\"Probe center not implemented yet\")\n",
    "                if \"WARNING\" in line and \"dt of frame\" and stimulus_ON:\n",
    "                    curr_stim.frame_delay.append(self._extract_delay(line))\n",
    "                    if curr_stim.frame_delay[-1][1] > 2000/60: #if longer than 2 frames could be bad\n",
    "                        print(curr_stim.name, \" \".join(line.split()[1:])[:-1])\n",
    "        return self.stimuli\n",
    "    \n",
    "class Stimulus:\n",
    "    \"\"\"Stimulus object containing information about it's presentation.\n",
    "        - start_time : a datetime object)\n",
    "        - stop_time : a datetime object)\n",
    "        - parameters : Parameters extracted from the QDSpy\n",
    "        - md5 : The md5 hash of that compiled version of the stimulus\n",
    "        - name : The name of the stimulus \n",
    "    \"\"\"\n",
    "    def __init__(self,start):\n",
    "        self.start_time = start\n",
    "        self.stop_time = None\n",
    "        self.parameters = {}\n",
    "        self.md5 = None\n",
    "        self.name = \"NoName\"\n",
    "        \n",
    "        self.frame_delay = []\n",
    "        self.is_aborted = False\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        self.parameters.update(parameters)\n",
    "        if \"_sName\" in parameters.keys():\n",
    "            self.name = parameters[\"_sName\"]\n",
    "        if \"stimMD5\" in parameters.keys():\n",
    "            self.md5 = parameters[\"stimMD5\"]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"%s %s at %s\" %(self.name+\" \"*(24-len(self.name)),self.md5,self.start_time)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read QDSpy logs of your experiment, simply provide the folder containing the log you want to read to `get_QDSpy_logs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flickering_bars_pr WARNING dt of frame #15864 was 50.315 m\n",
      "flickering_bars_pr WARNING dt of frame #19477 was 137.235 m\n"
     ]
    }
   ],
   "source": [
    "#logs = get_QDSpy_logs(\"./files/basic_synchro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns a list fo the QDSpy logs. Stimuli are contained in a list inside each log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NoName                   0a3053a663e357056b5b3c55856f8c88 at 2020-03-31 16:57:19,\n",
       " set_background           3e93aba4e4b7bee28aaeb6c6294d9dfe at 2020-03-31 17:08:54,\n",
       " checkerboard             eed21bda540934a428e93897908d049e at 2020-03-31 17:09:26,\n",
       " chirp_am                 31ca901daf66fcc4a832ed489d37de31 at 2020-03-31 17:24:33,\n",
       " chirp_freq_epoch         89c7c36d0071f4f4bef2643086ae9b95 at 2020-03-31 17:27:45,\n",
       " flickering_bars_pr       0049591cdf7aa379a458230e84cc3eec at 2020-03-31 17:30:25,\n",
       " fullfield_flicker        1424ca0ff1e33a87d9072a8dd42a628a at 2020-03-31 17:45:32,\n",
       " moving_gratings          4e9001c1674d1851e7748b37bd1b81a4 at 2020-03-31 17:50:36]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logs[0].stimuli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stimuli objects contains informations on how their display went: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flickering_bars_pr 2020-03-31 17:30:25 [(15864, 50.315), (19477, 137.235)] 0049591cdf7aa379a458230e84cc3eec\n"
     ]
    }
   ],
   "source": [
    "# stim = logs[0].stimuli[5]\n",
    "# print(stim.name, stim.start_time, stim.frame_delay, stim.md5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def unpack_stim_npy(npy_dir, md5_hash):\n",
    "    \"\"\"Find the stimuli of a given hash key in the npy stimulus folder. The stimuli are in a compressed version\n",
    "    comprising three files. inten for the stimulus values on the screen, marker for the values of the marker\n",
    "    read by a photodiode to get the stimulus timing during a record, and an optional shader that is used to\n",
    "    specify informations about a shader when used, like for the moving gratings.\"\"\"\n",
    "    \n",
    "    #Stimuli can be either npy or npz (useful when working remotely)\n",
    "    def find_file(ftype):\n",
    "        flist = glob.glob(os.path.join(npy_dir, \"*_\"+ftype+\"_\"+md5_hash+\".npy\"))\n",
    "        if len(flist)==0:\n",
    "            flist = glob.glob(os.path.join(npy_dir, \"*_\"+ftype+\"_\"+md5_hash+\".npz\"))\n",
    "            res = np.load(flist[0])[\"arr_0\"]\n",
    "        else:\n",
    "            res = np.load(flist[0])\n",
    "        return res\n",
    "    \n",
    "    inten  = find_file(\"intensities\")\n",
    "    marker = find_file(\"marker\")\n",
    "    \n",
    "    shader, unpack_shader = None, None\n",
    "    if len(glob.glob(os.path.join(npy_dir, \"*_shader_\"+md5_hash+\".np*\")))>0:\n",
    "        shader        = find_file(\"shader\")\n",
    "        unpack_shader = np.empty((np.sum(marker[:,0]), *shader.shape[1:]))\n",
    "\n",
    "    #The latter unpacks the arrays\n",
    "    unpack_inten  = np.empty((np.sum(marker[:,0]), *inten.shape[1:]))\n",
    "    unpack_marker = np.empty(np.sum(marker[:,0]))\n",
    "\n",
    "    cursor = 0\n",
    "    for i, n_frame in enumerate(marker[:,0]):\n",
    "        unpack_inten[cursor:cursor+n_frame] = inten[i]\n",
    "        unpack_marker[cursor:cursor+n_frame] = marker[i, 1]\n",
    "        if shader is not None:\n",
    "            unpack_shader[cursor:cursor+n_frame] = shader[i]\n",
    "        cursor += n_frame\n",
    "    \n",
    "    return unpack_inten, unpack_marker, unpack_shader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flickering_bars_pr WARNING dt of frame #15864 was 50.315 m\n",
      "flickering_bars_pr WARNING dt of frame #19477 was 137.235 m\n"
     ]
    }
   ],
   "source": [
    "# logs = get_QDSpy_logs(\"./files/basic_synchro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To unpack the stimulus values, provide the folder of the numpy arrays and the hash of the stimulus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpacked = unpack_stim_npy(\"./files/basic_synchro/stimulus_data\", \"eed21bda540934a428e93897908d049e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpacked is a tuple, where the first element is the intensity of shape (n_frames, n_colors, y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54039, 1, 18, 32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unpacked[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second element of the tuple repesents the marker values for the timing. QDSpy defaults are zero and ones, but I used custom red squares taking intensities [50,100,150,200,250] to time with five different signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 4., 0., 4., 4., 4., 0., 4., 0., 4., 4., 4.,\n",
       "       0., 4., 0., 4., 0., 0., 0., 0., 0., 0., 4., 0., 1., 2., 3., 4., 0.,\n",
       "       1., 2., 3., 4., 0., 1., 2., 3., 4., 0., 1., 2., 3., 4., 0., 1.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unpacked[1][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each stimulus is also starting with a barcode, of the form:\n",
    "\n",
    "0 0 0 0 0 0 4 0 4\\*[1-4] 0 4\\*[1-4] 0 4\\*[1-4] 0 4\\*[1-4] 0 4 0 0 0 0 0 0 \n",
    "\n",
    "and ends with 0 0 0 0 0 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extract_spyking_circus_results(dir_, record_basename):\n",
    "    \"\"\"Extract the good cells of a record. Overlap with phy_results_dict.\"\"\"\n",
    "    phy_dir  = os.path.join(dir_,record_basename+\"/\"+record_basename+\".GUI\")\n",
    "    phy_dict = phy_results_dict(phy_dir)\n",
    "    \n",
    "    good_clusters = []\n",
    "    with open(os.path.join(phy_dir,'cluster_group.tsv'), 'r') as tsvfile:\n",
    "        spamreader = csv.reader(tsvfile, delimiter='\\t', quotechar='|')\n",
    "        for i,row in enumerate(spamreader):\n",
    "            if row[1] == \"good\":\n",
    "                good_clusters.append(int(row[0]))\n",
    "    good_clusters = np.array(good_clusters)\n",
    "    \n",
    "    phy_dict[\"good_clusters\"] = good_clusters\n",
    "    \n",
    "    return phy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extract_best_pupil(fn):\n",
    "    \"\"\"From results of MaskRCNN, go over all or None pupil detected and select the best pupil.\n",
    "    Each pupil returned is (x,y,width,height,angle,probability)\"\"\"\n",
    "    pupil = np.load(fn, allow_pickle=True)\n",
    "    filtered_pupil = np.empty((len(pupil), 6))\n",
    "    for i, detected in enumerate(pupil):\n",
    "        if len(detected)>0:\n",
    "            best = detected[0]\n",
    "            for detect in detected[1:]:\n",
    "                if detect[5]>best[5]:\n",
    "                    best = detect\n",
    "            filtered_pupil[i] = np.array(best)\n",
    "        else:\n",
    "            filtered_pupil[i] = np.array([0,0,0,0,0,0])\n",
    "    return filtered_pupil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def stack_len_extraction(stack_info_dir):\n",
    "    \"\"\"Extract from ImageJ macro directives the size of the stacks acquired.\"\"\"\n",
    "    ptrn_nFrame = r\".*number=(\\d*) .*\"\n",
    "    l_epochs = []\n",
    "    for fn in glob.glob(os.path.join(stack_info_dir, \"*.txt\")):\n",
    "        with open(fn) as f:\n",
    "            line = f.readline()\n",
    "            l_epochs.append(int(re.findall(ptrn_nFrame, line)[0]))\n",
    "    return l_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_utils.ipynb.\n",
      "Converted 02_processing.ipynb.\n",
      "Converted 03_modelling.ipynb.\n",
      "Converted 04_plotting.ipynb.\n",
      "Converted 05_database.ipynb.\n",
      "Converted 10_synchro.io.ipynb.\n",
      "Converted 11_synchro.extracting.ipynb.\n",
      "Converted 12_synchro.processing.ipynb.\n",
      "Converted 99_testdata.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
