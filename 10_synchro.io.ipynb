{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp synchro.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IO classes are adapted from the SpykingCircus project by Pierre Yger and Olivier Marre https://spyking-circus.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import re, sys, os, logging, struct\n",
    "import h5py\n",
    "from colorama import Fore\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    '''\n",
    "    return [atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "def filter_per_extension(files, extension):\n",
    "    results = []\n",
    "    for file in files:\n",
    "        fn, ext = os.path.splitext(file)\n",
    "        if ext == extension:\n",
    "            results += [file]\n",
    "    return results\n",
    "\n",
    "def print_and_log(to_print, level='info', logger=None, display=True):\n",
    "    if display:\n",
    "        if level == 'default':\n",
    "            for line in to_print:\n",
    "                print(Fore.WHITE + line + '\\r')\n",
    "        if level == 'info':\n",
    "            print_info(to_print)\n",
    "        elif level == 'error':\n",
    "            print_error(to_print)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def print_info(lines):\n",
    "    \"\"\"Prints informations messages, enhanced graphical aspects.\"\"\"\n",
    "    print(Fore.YELLOW + \"-------------------------  Informations  -------------------------\\r\")\n",
    "    for line in lines:\n",
    "        print(Fore.YELLOW + \"| \" + line + '\\r')\n",
    "    print(Fore.YELLOW + \"------------------------------------------------------------------\\r\" + Fore.WHITE)\n",
    "\n",
    "def print_error(lines):\n",
    "    \"\"\"Prints errors messages, enhanced graphical aspects.\"\"\"\n",
    "    print(Fore.RED + \"----------------------------  Error  -----------------------------\\r\")\n",
    "    for line in lines:\n",
    "        print(Fore.RED + \"| \" + line + '\\r')\n",
    "    print(Fore.RED + \"------------------------------------------------------------------\\r\" + Fore.WHITE)\n",
    "\n",
    "\n",
    "def get_offset(data_dtype, dtype_offset):\n",
    "\n",
    "    if dtype_offset == 'auto':\n",
    "        if data_dtype in ['uint16', np.uint16]:\n",
    "            dtype_offset = 32768\n",
    "        elif data_dtype in ['int16', np.int16]:\n",
    "            dtype_offset = 0\n",
    "        elif data_dtype in ['int32', np.int32]:\n",
    "            dtype_offset = 0\n",
    "        elif data_dtype in ['int64', np.int64]:\n",
    "            dtype_offset = 0\n",
    "        elif data_dtype in ['float32', np.float32]:\n",
    "            dtype_offset = 0\n",
    "        elif data_dtype in ['int8', np.int8]:\n",
    "            dtype_offset = 0\n",
    "        elif data_dtype in ['uint8', np.uint8]:\n",
    "            dtype_offset = 127\n",
    "        elif data_dtype in ['float64', np.float64]:\n",
    "            dtype_offset = 0\n",
    "        elif data_dtype==\">d\":\n",
    "            dtype_offset = 0\n",
    "    else:\n",
    "        try:\n",
    "            dtype_offset = int(dtype_offset)\n",
    "        except:\n",
    "            print(\"Offset %s is not valid\" %dtype_offset)\n",
    "\n",
    "    return dtype_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DataFile(object):\n",
    "\n",
    "    '''\n",
    "    A generic class that will represent how the program interacts with the data. Such an abstraction\n",
    "    layer should allow people to write their own wrappers, for several file formats, with or without\n",
    "    parallel write, streams, and so on. Note that depending on the complexity of the datastructure,\n",
    "    this extra layer can slow down the code.\n",
    "    '''\n",
    "\n",
    "    description      = \"mydatafile\"     # Description of the file format\n",
    "    extension        = [\".myextension\"] # extensions\n",
    "    parallel_write   = False            # can be written in parallel (using the comm object)\n",
    "    is_writable      = False            # can be written\n",
    "    is_streamable    = ['multi-files']  # If the file format can support streams of data ['multi-files' is a default, but can be something else]\n",
    "    _shape           = None             # The total shape of the data (nb time steps, nb channels) accross streams if any\n",
    "    _t_start         = None             # The global t_start of the data\n",
    "    _t_stop          = None             # The final t_stop of the data, accross all streams if any\n",
    "\n",
    "    # This is a dictionary of values that need to be provided to the constructor, with the corresponding type\n",
    "    _required_fields = {}\n",
    "\n",
    "    # This is a dictionary of values that may have a default value, if not provided to the constructor\n",
    "    _default_values  = {}\n",
    "\n",
    "    _params          = {}\n",
    "\n",
    "    def __init__(self, file_name, params, is_empty=False, stream_mode=None):\n",
    "        '''\n",
    "        The constructor that will create the DataFile object. Note that by default, values are read from the header\n",
    "        of the file. If not found in the header, they are read from the parameter file. If no values are found, the\n",
    "        code will trigger an error\n",
    "\n",
    "        What you need to specify at a generic level (for a given file format)\n",
    "            - parallel_write  : can the file be safely written in parallel ?\n",
    "            - is_writable     : if the file can be written\n",
    "            - is_streamable   : if the file format can support streaming data\n",
    "            - required_fields : what parameter must be specified for the file format, along with the type\n",
    "            - default_values  : parameters that may have default values if not provided\n",
    "\n",
    "        What you need to specify at a low level (maybe by getting specific values with _read_from_header)\n",
    "            - _shape          : the size of the data, should be a tuple (duration in time bins, nb_channels)\n",
    "            - _t_start        : the time (in time steps) of the recording (0 by default)\n",
    "        '''\n",
    "\n",
    "        self.params = {}\n",
    "        self.params.update(self._params)\n",
    "\n",
    "        if not is_empty:\n",
    "            self._check_filename(file_name)\n",
    "\n",
    "        if stream_mode is not None:\n",
    "            self.is_stream = True\n",
    "            if not stream_mode in self.is_streamable:\n",
    "                if self.is_master:\n",
    "                    print_and_log([\"The file format %s does not support stream mode %s\" %(self.description, stream_mode)], 'error', logger)\n",
    "                sys.exit(1)\n",
    "            if is_empty:\n",
    "                sys.exit(1)\n",
    "        else:\n",
    "            self.is_stream = False\n",
    "\n",
    "        self.file_name   = file_name\n",
    "        self.is_empty    = is_empty\n",
    "        self.stream_mode = stream_mode\n",
    "\n",
    "        f_next, extension = os.path.splitext(self.file_name)\n",
    "\n",
    "        self._check_extension(extension)\n",
    "        self._fill_from_params(params)\n",
    "\n",
    "        if not self.is_empty:\n",
    "            try:\n",
    "                self._fill_from_header(self._read_from_header())\n",
    "            except Exception as ex:\n",
    "                print_and_log([\"There is an error in the _read_from_header method of the wrapper\\n\" + str(ex)], 'error', logger)\n",
    "        else:\n",
    "            self._shape = (0, 0)\n",
    "\n",
    "        if self._shape is None:\n",
    "            sys.exit(1)\n",
    "\n",
    "        self.params['dtype_offset'] = get_offset(self.data_dtype, self.dtype_offset)\n",
    "\n",
    "        if self.stream_mode:\n",
    "            self._sources = self.set_streams(self.stream_mode)\n",
    "            self._times   = []\n",
    "            for source in self._sources:\n",
    "                self._times += [source.t_start]\n",
    "            print_and_log(['The file is composed of %d streams' %len(self._sources),\n",
    "                           'Times are between %d and %d' %(self._sources[0].t_start, self._sources[-1].t_stop)], 'debug',logger)\n",
    "\n",
    "    ##################################################################################################################\n",
    "    ##################################################################################################################\n",
    "    #########                  Methods that need to be overwritten for a given fileformat                      #######\n",
    "    ##################################################################################################################\n",
    "    ##################################################################################################################\n",
    "\n",
    "\n",
    "    def _read_from_header(self):\n",
    "        '''\n",
    "            This function is called only if the file is not empty, and should fill the values in the constructor\n",
    "            such as _shape. It returns a dictionnary, that will be added to self._params based on the constrains given by\n",
    "            required_fields and default_values\n",
    "        '''\n",
    "        raise NotImplementedError('The _read_from_header method needs to be implemented for file format %s' %self.description)\n",
    "\n",
    "\n",
    "    def _open(self, mode=''):\n",
    "        '''\n",
    "            This function should open the file\n",
    "            - mode can be to read only 'r', or to write 'w'\n",
    "        '''\n",
    "        raise NotImplementedError('The open method needs to be implemented for file format %s' %self.description)\n",
    "\n",
    "\n",
    "    def _close(self):\n",
    "        '''\n",
    "            This function closes the file\n",
    "        '''\n",
    "        raise NotImplementedError('The close method needs to be implemented for file format %s' %self.description)\n",
    "\n",
    "\n",
    "    def read_chunk(self, idx, chunk_size, padding=(0, 0), nodes=None):\n",
    "        '''\n",
    "        Assuming the analyze function has been called before, this is the main function\n",
    "        used by the code, in all steps, to get data chunks. More precisely, assuming your\n",
    "        dataset can be divided in nb_chunks (see analyze) of temporal size (chunk_size),\n",
    "\n",
    "            - idx is the index of the chunk you want to load\n",
    "            - chunk_size is the time of those chunks, in time steps\n",
    "            - if the data loaded are data[idx:idx+1], padding should add some offsets,\n",
    "                in time steps, such that we can load data[idx+padding[0]:idx+padding[1]]\n",
    "            - nodes is a list of nodes, between 0 and nb_channels\n",
    "        '''\n",
    "\n",
    "        raise NotImplementedError('The read_chunk method needs to be implemented for file format %s' %self.description)\n",
    "\n",
    "    def read_chunk_adc(self, idx, chunk_size, padding=(0, 0), nodes=None):\n",
    "        '''\n",
    "        Same as read_chunk, but for the analog channel of the file.\n",
    "\n",
    "            - idx is the index of the chunk you want to load\n",
    "            - chunk_size is the time of those chunks, in time steps\n",
    "            - if the data loaded are data[idx:idx+1], padding should add some offsets,\n",
    "                in time steps, such that we can load data[idx+padding[0]:idx+padding[1]]\n",
    "            - nodes is a list of nodes, between 0 and nb_channels\n",
    "        '''\n",
    "\n",
    "        raise NotImplementedError('The read_chunk_adc method needs to be implemented for file format %s' %self.description)\n",
    "\n",
    "\n",
    "    def write_chunk(self, time, data):\n",
    "        '''\n",
    "            This function writes data at a given time.\n",
    "            - time is expressed in timestep\n",
    "            - data must be a 2D matrix of size time_length x nb_channels\n",
    "        '''\n",
    "        raise NotImplementedError('The set_data method needs to be implemented for file format %s' %self.description)\n",
    "\n",
    "\n",
    "    def set_streams(self, stream_mode):\n",
    "        '''\n",
    "            This function is only used for file format supporting streams, and need to return a list of datafiles, with\n",
    "            appropriate t_start for each of them. Note that the results will be using the times defined by the streams.\n",
    "            You can do anything regarding the keyword used for the stream mode, but multi-files is immplemented by default\n",
    "            This will allow every file format to be streamed from multiple sources, and processed as a single file.\n",
    "        '''\n",
    "\n",
    "        if stream_mode == 'multi-files':\n",
    "            dirname         = os.path.abspath(os.path.dirname(self.file_name))\n",
    "            fname           = os.path.basename(self.file_name)\n",
    "            fn, ext         = os.path.splitext(fname)\n",
    "            all_files       = os.listdir(dirname)\n",
    "            all_files       = filter_per_extension(all_files, ext)\n",
    "            all_files.sort(key=natural_keys)\n",
    "\n",
    "            sources         = []\n",
    "            to_write        = []\n",
    "            global_time     = 0\n",
    "            params          = self.get_description()\n",
    "\n",
    "            for fname in all_files:\n",
    "                new_data   = type(self)(os.path.join(os.path.abspath(dirname), fname), params)\n",
    "                new_data._t_start = global_time\n",
    "                global_time += new_data.duration\n",
    "                sources     += [new_data]\n",
    "                to_write    += ['We found the datafile %s with t_start %s and duration %s' %(new_data.file_name, new_data.t_start, new_data.duration)]\n",
    "            print_and_log(to_write, 'debug', logger)\n",
    "            return sources\n",
    "\n",
    "    ################################## Optional, only if internal names are changed ##################################\n",
    "\n",
    "    @property\n",
    "    def sampling_rate(self):\n",
    "        return self.params['sampling_rate']\n",
    "\n",
    "    @property\n",
    "    def data_dtype(self):\n",
    "        return self.params['data_dtype']\n",
    "\n",
    "    @property\n",
    "    def dtype_offset(self):\n",
    "        return self.params['dtype_offset']\n",
    "\n",
    "    @property\n",
    "    def data_offset(self):\n",
    "        return self.params['data_offset']\n",
    "\n",
    "    @property\n",
    "    def nb_channels(self):\n",
    "        return int(self.params['nb_channels'])\n",
    "\n",
    "    @property\n",
    "    def gain(self):\n",
    "        return self.params['gain']\n",
    "\n",
    "    ##################################################################################################################\n",
    "    ##################################################################################################################\n",
    "    #########           End of methods that need to be overwritten for a given fileformat                      #######\n",
    "    ##################################################################################################################\n",
    "    ##################################################################################################################\n",
    "\n",
    "\n",
    "    def get_file_names(self):\n",
    "        res = []\n",
    "        if self.stream_mode == 'multi-files':\n",
    "            for source in self._sources:\n",
    "                res += [source.file_name]\n",
    "        return res\n",
    "\n",
    "    def _check_filename(self, file_name):\n",
    "        if not os.path.exists(file_name):\n",
    "            sys.exit(1)\n",
    "\n",
    "\n",
    "    def _check_extension(self, extension):\n",
    "        if len(self.extension) > 0:\n",
    "            if not extension in self.extension + [item.upper() for item in self.extension]:\n",
    "                sys.exit(1)\n",
    "\n",
    "\n",
    "    def _fill_from_params(self, params):\n",
    "\n",
    "        for key in self._required_fields:\n",
    "            if key not in params:\n",
    "                self._check_requirements_(params)\n",
    "            else:\n",
    "                self.params[key] = self._required_fields[key](params[key])\n",
    "\n",
    "        for key in self._default_values:\n",
    "            if key not in params:\n",
    "                self.params[key] = self._default_values[key]\n",
    "            else:\n",
    "                self.params[key] = type(self._default_values[key])(params[key])\n",
    "\n",
    "    def _fill_from_header(self, header):\n",
    "\n",
    "        for key in list(header.keys()):\n",
    "            self.params[key] = header[key]\n",
    "\n",
    "    def _check_requirements_(self, params):\n",
    "\n",
    "        missing = {}\n",
    "\n",
    "        for key, value in list(self._required_fields.items()):\n",
    "            if key not in list(params.keys()):\n",
    "                missing[key] = value\n",
    "\n",
    "        if len(missing) > 0:\n",
    "            self._display_requirements_()\n",
    "            sys.exit(1)\n",
    "\n",
    "\n",
    "    def _display_requirements_(self):\n",
    "\n",
    "        to_write  = ['The parameters for %s file format are:' %self.description.upper(), '']\n",
    "        nb_params = 0\n",
    "\n",
    "        for key, value in list(self._required_fields.items()):\n",
    "            mystring  = '-- %s -- %s' %(key, str(value))\n",
    "            mystring  += ' [** mandatory **]'\n",
    "            to_write  += [mystring]\n",
    "            nb_params += 1\n",
    "\n",
    "        to_write += ['']\n",
    "\n",
    "        for key, value in list(self._default_values.items()):\n",
    "            mystring  = '-- %s -- %s' %(key, str(type(value)))\n",
    "            mystring  += ' [default is %s]' %value\n",
    "            to_write  += [mystring]\n",
    "            nb_params += 1\n",
    "\n",
    "    def _scale_data_to_float32(self, data):\n",
    "        '''\n",
    "            This function will convert data from local data dtype into float32, the default format of the algorithm\n",
    "        '''\n",
    "        if self.data_dtype != np.float32:\n",
    "            data  = data.astype(np.float32)\n",
    "\n",
    "        if self.dtype_offset != 0:\n",
    "            data  -= self.dtype_offset\n",
    "\n",
    "        if np.any(self.gain != 1):\n",
    "            data *= self.gain\n",
    "\n",
    "        return np.ascontiguousarray(data)\n",
    "\n",
    "\n",
    "    def _unscale_data_from_float32(self, data):\n",
    "        '''\n",
    "            This function will convert data from float32 back to the original format of the file\n",
    "        '''\n",
    "\n",
    "        if np.any(self.gain != 1):\n",
    "            data /= self.gain\n",
    "\n",
    "        if self.dtype_offset != 0:\n",
    "            data  += self.dtype_offset\n",
    "\n",
    "        if (data.dtype != self.data_dtype) and (self.data_dtype != np.float32):\n",
    "            data = data.astype(self.data_dtype)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "    def _count_chunks(self, chunk_size, duration, strict=False):\n",
    "        '''\n",
    "            This function will count how many block of size chunk_size can be found within a certain duration\n",
    "            This returns the number of blocks, plus the remaining part\n",
    "        '''\n",
    "        nb_chunks      = duration // chunk_size\n",
    "        last_chunk_len = duration - nb_chunks * chunk_size\n",
    "\n",
    "        if not strict and last_chunk_len > 0:\n",
    "            nb_chunks += 1\n",
    "\n",
    "        return nb_chunks, last_chunk_len\n",
    "\n",
    "\n",
    "    def _get_t_start_t_stop(self, idx, chunk_size, padding=(0,0)):\n",
    "\n",
    "        t_start     = idx*np.int64(chunk_size)+padding[0]\n",
    "        t_stop      = (idx+1)*np.int64(chunk_size)+padding[1]\n",
    "\n",
    "        if t_stop > self.duration:\n",
    "            t_stop = self.duration\n",
    "\n",
    "        if t_start < 0:\n",
    "            t_start = 0\n",
    "\n",
    "        return t_start, t_stop\n",
    "\n",
    "\n",
    "    def _get_streams_index_by_time(self, local_time):\n",
    "        if self.is_stream:\n",
    "            cidx  = np.searchsorted(self._times, local_time, 'right') - 1\n",
    "            return cidx\n",
    "\n",
    "    def is_first_chunk(self, idx, nb_chunks):\n",
    "\n",
    "        if self.is_stream:\n",
    "            cidx = np.searchsorted(self._chunks_in_sources, idx, 'right') - 1\n",
    "            idx -= self._chunks_in_sources[cidx]\n",
    "            if idx == 0:\n",
    "                return True\n",
    "        else:\n",
    "            if idx == 0:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def is_last_chunk(self, idx, nb_chunks):\n",
    "\n",
    "        if self.is_stream:\n",
    "            if (idx > 0) and (idx in self._chunks_in_sources - 1):\n",
    "                return True\n",
    "        else:\n",
    "            if idx == nb_chunks:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_snippet(self, global_time, length, nodes=None):\n",
    "        '''\n",
    "            This function should return a time snippet of size length x nodes\n",
    "            - time is in timestep\n",
    "            - length is in timestep\n",
    "            - nodes is a list of nodes, between 0 and nb_channels\n",
    "        '''\n",
    "        if self.is_stream:\n",
    "            cidx = self._get_streams_index_by_time(global_time)\n",
    "            return self._sources[cidx].get_snippet(global_time, length, nodes)\n",
    "        else:\n",
    "            local_time = global_time - self.t_start\n",
    "            return self.get_data(0, chunk_size=length, padding=(local_time, local_time), nodes=nodes)[0]\n",
    "            \n",
    "    def get_snippet_adc(self, global_time, length, nodes=None):\n",
    "        '''\n",
    "            This function should return a time snippet of size length x nodes\n",
    "            - time is in timestep\n",
    "            - length is in timestep\n",
    "            - nodes is a list of nodes, between 0 and nb_channels\n",
    "        '''\n",
    "        if self.is_stream:\n",
    "            cidx = self._get_streams_index_by_time(global_time)\n",
    "            return self._sources[cidx].get_snippet_adc(global_time, length, nodes)\n",
    "        else:\n",
    "            local_time = global_time - self.t_start\n",
    "            return self.get_data_adc(0, chunk_size=length, padding=(local_time, local_time), nodes=nodes)[0]\n",
    "\n",
    "\n",
    "    def get_data(self, idx, chunk_size, padding=(0, 0), nodes=None):\n",
    "\n",
    "        if self.is_stream:\n",
    "            cidx = np.searchsorted(self._chunks_in_sources, idx, 'right') - 1\n",
    "            idx -= self._chunks_in_sources[cidx]\n",
    "            return self._sources[cidx].read_chunk(idx, chunk_size, padding, nodes), self._sources[cidx].t_start + idx*chunk_size\n",
    "        else:\n",
    "            return self.read_chunk(idx, chunk_size, padding, nodes), self.t_start + idx*chunk_size\n",
    "            \n",
    "    def get_data_adc(self, idx, chunk_size, padding=(0, 0), nodes=None):\n",
    "\n",
    "        if self.is_stream:\n",
    "            cidx = np.searchsorted(self._chunks_in_sources, idx, 'right') - 1\n",
    "            idx -= self._chunks_in_sources[cidx]\n",
    "            return self._sources[cidx].read_chunk_adc(idx, chunk_size, padding, nodes), self._sources[cidx].t_start + idx*chunk_size\n",
    "        else:\n",
    "            return self.read_chunk_adc(idx, chunk_size, padding, nodes), self.t_start + idx*chunk_size\n",
    "\n",
    "\n",
    "    def set_data(self, global_time, data):\n",
    "\n",
    "        if self.is_stream:\n",
    "            cidx = self._get_streams_index_by_time(global_time)\n",
    "            local_time = global_time - self._sources[cidx].t_start\n",
    "            return self._sources[cidx].write_chunk(local_time, data)\n",
    "        else:\n",
    "            local_time = global_time - self.t_start\n",
    "            return self.write_chunk(local_time, data)\n",
    "\n",
    "\n",
    "    def analyze(self, chunk_size, strict=False):\n",
    "        '''\n",
    "            This function should return two values:\n",
    "            - the number of temporal chunks of temporal size chunk_size that can be found\n",
    "            in the data. Note that even if the last chunk is not complete, it has to be\n",
    "            counted. chunk_size is expressed in time steps\n",
    "            - the length of the last uncomplete chunk, in time steps\n",
    "        '''\n",
    "        if self.is_stream:\n",
    "            nb_chunks               = 0\n",
    "            last_chunk_len          = 0\n",
    "            self._chunks_in_sources = [0]\n",
    "\n",
    "            for source in self._sources:\n",
    "                a, b            = self._count_chunks(chunk_size, source.duration, strict)\n",
    "                nb_chunks      += a\n",
    "                last_chunk_len += b\n",
    "\n",
    "                self._chunks_in_sources += [nb_chunks]\n",
    "\n",
    "            self._chunks_in_sources = np.array(self._chunks_in_sources)\n",
    "\n",
    "            return nb_chunks, last_chunk_len\n",
    "        else:\n",
    "            return self._count_chunks(chunk_size, self.duration, strict)\n",
    "\n",
    "\n",
    "    def get_description(self):\n",
    "        result = {}\n",
    "        for key in ['sampling_rate', 'data_dtype', 'gain', 'nb_channels', 'dtype_offset'] + list(self._default_values.keys()) + list(self._required_fields.keys()):\n",
    "            result[key] = self.params[key]\n",
    "        return result\n",
    "\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return (self.duration, int(self.nb_channels))\n",
    "\n",
    "\n",
    "    @property\n",
    "    def duration(self):\n",
    "        if self.is_stream:\n",
    "            duration = 0\n",
    "            for source in self._sources:\n",
    "                duration += source.duration\n",
    "            return duration\n",
    "        else:\n",
    "            return np.int64(self._shape[0])\n",
    "\n",
    "\n",
    "    @property\n",
    "    def is_master(self):\n",
    "        return True#comm.rank == 0\n",
    "\n",
    "\n",
    "    @property\n",
    "    def t_start(self):\n",
    "        if self.is_stream:\n",
    "            return self._sources[0].t_start\n",
    "        else:\n",
    "            if self._t_start is None:\n",
    "                self._t_start = 0\n",
    "            return self._t_start\n",
    "\n",
    "\n",
    "    @property\n",
    "    def t_stop(self):\n",
    "        if self.is_stream:\n",
    "            return self._sources[-1].t_stop\n",
    "        else:\n",
    "            if self._t_stop is None:\n",
    "                self._t_stop = self.t_start + self.duration\n",
    "            return self._t_stop\n",
    "\n",
    "\n",
    "    @property\n",
    "    def nb_streams(self):\n",
    "        if self.is_stream:\n",
    "            return len(self._sources)\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def open(self, mode='r'):\n",
    "        if self.is_stream:\n",
    "            for source in self._sources:\n",
    "                source._open(mode)\n",
    "        else:\n",
    "            self._open(mode)\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        if self.is_stream:\n",
    "            for source in self._sources:\n",
    "                source._close()\n",
    "        else:\n",
    "            self._close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_header(fid):\n",
    "    \"\"\"Reads the Intan File Format header from the given file.\"\"\"\n",
    "\n",
    "    # Check 'magic number' at beginning of file to make sure this is an Intan\n",
    "    # Technologies RHD2000 data file.\n",
    "    magic_number, = struct.unpack('<I', fid.read(4))\n",
    "    if magic_number != int('c6912702', 16): raise Exception('Unrecognized file type.')\n",
    "\n",
    "    header = {}\n",
    "    # Read version number.\n",
    "    version = {}\n",
    "    (version['major'], version['minor']) = struct.unpack('<hh', fid.read(4))\n",
    "    header['version'] = version\n",
    "\n",
    "    freq = {}\n",
    "\n",
    "    # Read information of sampling rate and amplifier frequency settings.\n",
    "    header['sample_rate'], = struct.unpack('<f', fid.read(4))\n",
    "    (freq['dsp_enabled'], freq['actual_dsp_cutoff_frequency'], freq['actual_lower_bandwidth'], freq['actual_upper_bandwidth'],\n",
    "    freq['desired_dsp_cutoff_frequency'], freq['desired_lower_bandwidth'], freq['desired_upper_bandwidth']) = struct.unpack('<hffffff', fid.read(26))\n",
    "\n",
    "\n",
    "    # This tells us if a software 50/60 Hz notch filter was enabled during\n",
    "    # the data acquisition.\n",
    "    notch_filter_mode, = struct.unpack('<h', fid.read(2))\n",
    "    header['notch_filter_frequency'] = 0\n",
    "    if notch_filter_mode == 1:\n",
    "        header['notch_filter_frequency'] = 50\n",
    "    elif notch_filter_mode == 2:\n",
    "        header['notch_filter_frequency'] = 60\n",
    "    freq['notch_filter_frequency'] = header['notch_filter_frequency']\n",
    "\n",
    "    (freq['desired_impedance_test_frequency'], freq['actual_impedance_test_frequency']) = struct.unpack('<ff', fid.read(8))\n",
    "\n",
    "    note1 = read_qstring(fid)\n",
    "    note2 = read_qstring(fid)\n",
    "    note3 = read_qstring(fid)\n",
    "    header['notes'] = { 'note1' : note1, 'note2' : note2, 'note3' : note3}\n",
    "\n",
    "    # If data file is from GUI v1.1 or later, see if temperature sensor data was saved.\n",
    "    header['num_temp_sensor_channels'] = 0\n",
    "    if (version['major'] == 1 and version['minor'] >= 1) or (version['major'] > 1) :\n",
    "        header['num_temp_sensor_channels'], = struct.unpack('<h', fid.read(2))\n",
    "\n",
    "\n",
    "    # If data file is from GUI v1.3 or later, load eval board mode.\n",
    "    header['eval_board_mode'] = 0\n",
    "    if ((version['major'] == 1) and (version['minor'] >= 3)) or (version['major'] > 1) :\n",
    "        header['eval_board_mode'], = struct.unpack('<h', fid.read(2))\n",
    "\n",
    "    # Place frequency-related information in data structure. (Note: much of this structure is set above)\n",
    "    freq['amplifier_sample_rate'] = header['sample_rate']\n",
    "    freq['aux_input_sample_rate'] = header['sample_rate'] / 4\n",
    "    freq['supply_voltage_sample_rate'] = header['sample_rate'] / 60\n",
    "    freq['board_adc_sample_rate'] = header['sample_rate']\n",
    "    freq['board_dig_in_sample_rate'] = header['sample_rate']\n",
    "\n",
    "    header['frequency_parameters'] = freq\n",
    "\n",
    "    # Create structure arrays for each type of data channel.\n",
    "    header['spike_triggers'] = []\n",
    "    header['amplifier_channels'] = []\n",
    "    header['aux_input_channels'] = []\n",
    "    header['supply_voltage_channels'] = []\n",
    "    header['board_adc_channels'] = []\n",
    "    header['board_dig_in_channels'] = []\n",
    "    header['board_dig_out_channels'] = []\n",
    "\n",
    "    # Read signal summary from data file header.\n",
    "\n",
    "    if (header['version']['major'] > 1):\n",
    "        header['reference_channel'] = read_qstring(fid)\n",
    "\n",
    "    number_of_signal_groups, = struct.unpack('<h', fid.read(2))\n",
    "\n",
    "    for signal_group in range(0, number_of_signal_groups):\n",
    "        signal_group_name = read_qstring(fid)\n",
    "        signal_group_prefix = read_qstring(fid)\n",
    "        (signal_group_enabled, signal_group_num_channels, signal_group_num_amp_channels) = struct.unpack('<hhh', fid.read(6))\n",
    "\n",
    "        if (signal_group_num_channels > 0) and (signal_group_enabled > 0):\n",
    "            for signal_channel in range(0, signal_group_num_channels):\n",
    "                new_channel = {'port_name' : signal_group_name, 'port_prefix' : signal_group_prefix, 'port_number' : signal_group}\n",
    "                new_channel['native_channel_name'] = read_qstring(fid)\n",
    "                new_channel['custom_channel_name'] = read_qstring(fid)\n",
    "                (new_channel['native_order'], new_channel['custom_order'], signal_type, channel_enabled, new_channel['chip_channel'], new_channel['board_stream']) = struct.unpack('<hhhhhh', fid.read(12))\n",
    "                new_trigger_channel = {}\n",
    "                (new_trigger_channel['voltage_trigger_mode'], new_trigger_channel['voltage_threshold'], new_trigger_channel['digital_trigger_channel'], new_trigger_channel['digital_edge_polarity'])  = struct.unpack('<hhhh', fid.read(8))\n",
    "                (new_channel['electrode_impedance_magnitude'], new_channel['electrode_impedance_phase']) = struct.unpack('<ff', fid.read(8))\n",
    "\n",
    "                if channel_enabled:\n",
    "                    if signal_type == 0:\n",
    "                        header['amplifier_channels'].append(new_channel)\n",
    "                        header['spike_triggers'].append(new_trigger_channel)\n",
    "                    elif signal_type == 1:\n",
    "                        header['aux_input_channels'].append(new_channel)\n",
    "                    elif signal_type == 2:\n",
    "                        header['supply_voltage_channels'].append(new_channel)\n",
    "                    elif signal_type == 3:\n",
    "                        header['board_adc_channels'].append(new_channel)\n",
    "                    elif signal_type == 4:\n",
    "                        header['board_dig_in_channels'].append(new_channel)\n",
    "                    elif signal_type == 5:\n",
    "                        header['board_dig_out_channels'].append(new_channel)\n",
    "                    else:\n",
    "                        raise Exception('Unknown channel type.')\n",
    "\n",
    "\n",
    "    # Summarize contents of data file.\n",
    "    header['num_amplifier_channels'] = len(header['amplifier_channels'])\n",
    "    header['num_aux_input_channels'] = len(header['aux_input_channels'])\n",
    "    header['num_supply_voltage_channels'] = len(header['supply_voltage_channels'])\n",
    "    header['num_board_adc_channels'] = len(header['board_adc_channels'])\n",
    "    header['num_board_dig_in_channels'] = len(header['board_dig_in_channels'])\n",
    "    header['num_board_dig_out_channels'] = len(header['board_dig_out_channels'])\n",
    "\n",
    "    return header\n",
    "\n",
    "\n",
    "def get_bytes_per_data_block(header):\n",
    "    \"\"\"Calculates the number of bytes in each 60-sample datablock.\"\"\"\n",
    "\n",
    "    if (header['version']['major'] == 1):\n",
    "        num_samples_per_data_block = 60\n",
    "    else:\n",
    "        num_samples_per_data_block = 128\n",
    "\n",
    "    # Each data block contains 60 amplifier samples.\n",
    "    bytes_per_block = num_samples_per_data_block * 4  # timestamp data\n",
    "    bytes_per_block = bytes_per_block + num_samples_per_data_block * 2 * header['num_amplifier_channels']\n",
    "\n",
    "    # Auxiliary inputs are sampled 4x slower than amplifiers\n",
    "    bytes_per_block = bytes_per_block + (num_samples_per_data_block / 4) * 2 * header['num_aux_input_channels']\n",
    "\n",
    "    # Supply voltage is sampled 60x slower than amplifiers\n",
    "    bytes_per_block = bytes_per_block + 1 * 2 * header['num_supply_voltage_channels']\n",
    "\n",
    "    # Board analog inputs are sampled at same rate as amplifiers\n",
    "    bytes_per_block = bytes_per_block + num_samples_per_data_block * 2 * header['num_board_adc_channels']\n",
    "\n",
    "    # Board digital inputs are sampled at same rate as amplifiers\n",
    "    if header['num_board_dig_in_channels'] > 0:\n",
    "        bytes_per_block = bytes_per_block + num_samples_per_data_block * 2\n",
    "\n",
    "    # Board digital outputs are sampled at same rate as amplifiers\n",
    "    if header['num_board_dig_out_channels'] > 0:\n",
    "        bytes_per_block = bytes_per_block + num_samples_per_data_block * 2\n",
    "\n",
    "    # Temp sensor is sampled 60x slower than amplifiers\n",
    "    if header['num_temp_sensor_channels'] > 0:\n",
    "        bytes_per_block = bytes_per_block + 1 * 2 * header['num_temp_sensor_channels']\n",
    "\n",
    "    return bytes_per_block\n",
    "\n",
    "\n",
    "\n",
    "def read_qstring(fid):\n",
    "    \"\"\"Read Qt style QString.\n",
    "\n",
    "    The first 32-bit unsigned number indicates the length of the string (in bytes).\n",
    "    If this number equals 0xFFFFFFFF, the string is null.\n",
    "\n",
    "    Strings are stored as unicode.\n",
    "    \"\"\"\n",
    "\n",
    "    length, = struct.unpack('<I', fid.read(4))\n",
    "    if length == int('ffffffff', 16): return \"\"\n",
    "\n",
    "    if length > (os.fstat(fid.fileno()).st_size - fid.tell() + 1) :\n",
    "        print(length)\n",
    "        raise Exception('Length too long.')\n",
    "\n",
    "    # convert length from bytes to 16-bit Unicode words\n",
    "    length = int(length / 2)\n",
    "\n",
    "    data = []\n",
    "    for i in range(0, length):\n",
    "        c, = struct.unpack('<H', fid.read(2))\n",
    "        data.append(c)\n",
    "\n",
    "    if sys.version_info >= (3,0):\n",
    "        a = ''.join([chr(c) for c in data])\n",
    "    else:\n",
    "        a = ''.join([chr(c) for c in data])\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "class RHDFile(DataFile):\n",
    "\n",
    "    description    = \"rhd\"\n",
    "    extension      = [\".rhd\"]\n",
    "    parallel_write = True\n",
    "    is_writable    = True\n",
    "    is_streamable  = ['multi-files']\n",
    "\n",
    "    _required_fields = {}\n",
    "    _default_values  = {}\n",
    "\n",
    "    _params          = {'dtype_offset' : 'auto',\n",
    "                        'data_dtype'   : 'uint16',\n",
    "                        'gain'         : 0.195}\n",
    "\n",
    "    def _read_from_header(self):\n",
    "\n",
    "        header = {}\n",
    "\n",
    "        self.file  = open(self.file_name, 'rb')\n",
    "        full_header = read_header(self.file)\n",
    "        header['nb_channels']   = full_header['num_amplifier_channels']\n",
    "        header['sampling_rate'] = full_header['sample_rate']\n",
    "\n",
    "        if full_header['version']['major'] == 1:\n",
    "            self.SAMPLES_PER_RECORD = 60\n",
    "        else:\n",
    "            self.SAMPLES_PER_RECORD = 128\n",
    "        self.nb_channels_adc = full_header['num_board_adc_channels']\n",
    "        header['data_offset']   = self.file.tell()\n",
    "        data_present         = False\n",
    "        filesize             = os.path.getsize(self.file_name)\n",
    "        self.bytes_per_block = get_bytes_per_data_block(full_header)\n",
    "        self.block_offset    = self.SAMPLES_PER_RECORD * 4\n",
    "        self.block_size      = 2 * self.SAMPLES_PER_RECORD * header['nb_channels']\n",
    "        self.block_offset_adc = (self.block_offset + self.block_size +\n",
    "                                 (self.SAMPLES_PER_RECORD/4) * full_header['num_aux_input_channels'] * 2 +\n",
    "                                 2 * full_header['num_supply_voltage_channels'])\n",
    "        self.block_size_adc  = 2 * self.SAMPLES_PER_RECORD * self.nb_channels_adc\n",
    "        bytes_remaining      = filesize - self.file.tell()\n",
    "\n",
    "        self.bytes_per_block_div = self.bytes_per_block / 2\n",
    "        self.block_offset_div    = self.block_offset / 2\n",
    "        self.block_offset_div_adc= self.block_offset_adc / 2\n",
    "        self.block_size_div      = self.block_size / 2\n",
    "        self.block_size_div_adc  = self.block_size_adc / 2\n",
    "\n",
    "        if bytes_remaining > 0:\n",
    "            data_present = True\n",
    "        if bytes_remaining % self.bytes_per_block != 0:\n",
    "            print_and_log(['Something is wrong with file size : should have a whole number of data blocks'], 'error', logger)\n",
    "\n",
    "        num_data_blocks = int(bytes_remaining / self.bytes_per_block)\n",
    "        self.num_amplifier_samples = self.SAMPLES_PER_RECORD * num_data_blocks\n",
    "\n",
    "        self.size        = self.num_amplifier_samples\n",
    "        self._shape      = (self.size, header['nb_channels'])\n",
    "        self.file.close()\n",
    "\n",
    "        return header\n",
    "\n",
    "\n",
    "    def _get_slice_(self, t_start, t_stop):\n",
    "\n",
    "        x_beg = np.int64(t_start // self.SAMPLES_PER_RECORD)\n",
    "        r_beg = np.mod(t_start, self.SAMPLES_PER_RECORD)\n",
    "        x_end = np.int64(t_stop // self.SAMPLES_PER_RECORD)\n",
    "        r_end = np.mod(t_stop, self.SAMPLES_PER_RECORD)\n",
    "\n",
    "        if x_beg == x_end:\n",
    "            g_offset = x_beg * self.bytes_per_block_div + self.block_offset_div\n",
    "            data_slice = np.arange(g_offset + r_beg * self.nb_channels, g_offset + r_end * self.nb_channels, dtype=np.int64)\n",
    "            yield data_slice\n",
    "        else:\n",
    "            for count, nb_blocks in enumerate(np.arange(x_beg, x_end + 1, dtype=np.int64)):\n",
    "                g_offset = nb_blocks * self.bytes_per_block_div + self.block_offset_div\n",
    "                if count == 0:\n",
    "                    data_slice = np.arange(g_offset + r_beg * self.nb_channels, g_offset + self.block_size_div, dtype=np.int64)\n",
    "                elif (count == (x_end - x_beg)):\n",
    "                    data_slice = np.arange(g_offset, g_offset + r_end * self.nb_channels, dtype=np.int64)\n",
    "                else:\n",
    "                    data_slice = np.arange(g_offset, g_offset + self.block_size_div, dtype=np.int64)\n",
    "                yield data_slice\n",
    "\n",
    "    def _get_slice_adc_(self, t_start, t_stop):\n",
    "\n",
    "        x_beg = np.int64(t_start // self.SAMPLES_PER_RECORD)\n",
    "        r_beg = np.mod(t_start, self.SAMPLES_PER_RECORD)\n",
    "        x_end = np.int64(t_stop // self.SAMPLES_PER_RECORD)\n",
    "        r_end = np.mod(t_stop, self.SAMPLES_PER_RECORD)\n",
    "\n",
    "        if x_beg == x_end:\n",
    "            g_offset = x_beg * self.bytes_per_block_div + self.block_offset_div_adc\n",
    "            data_slice = np.arange(g_offset + r_beg * self.nb_channels_adc, g_offset + r_end * self.nb_channels_adc, dtype=np.int64)\n",
    "            yield data_slice\n",
    "        else:\n",
    "            for count, nb_blocks in enumerate(np.arange(x_beg, x_end + 1, dtype=np.int64)):\n",
    "                g_offset = nb_blocks * self.bytes_per_block_div + self.block_offset_div_adc\n",
    "                if count == 0:\n",
    "                    data_slice = np.arange(g_offset + r_beg * self.nb_channels_adc, g_offset + self.block_size_div_adc, dtype=np.int64)\n",
    "                elif (count == (x_end - x_beg)):\n",
    "                    data_slice = np.arange(g_offset, g_offset + r_end * self.nb_channels_adc, dtype=np.int64)\n",
    "                else:\n",
    "                    data_slice = np.arange(g_offset, g_offset + self.block_size_div_adc, dtype=np.int64)\n",
    "                yield data_slice\n",
    "\n",
    "\n",
    "    def read_chunk(self, idx, chunk_size, padding=(0, 0), nodes=None):\n",
    "\n",
    "        t_start, t_stop = self._get_t_start_t_stop(idx, chunk_size, padding)\n",
    "        local_shape     = t_stop - t_start\n",
    "\n",
    "        local_chunk = np.zeros((self.nb_channels, local_shape), dtype=self.data_dtype)\n",
    "        data_slice  = self._get_slice_(t_start, t_stop)\n",
    "\n",
    "        self._open()\n",
    "        count = 0\n",
    "\n",
    "        for s in data_slice:\n",
    "            t_slice = len(s)//self.nb_channels\n",
    "            local_chunk[:, count:count + t_slice] = self.data[s].reshape(self.nb_channels, len(s)//self.nb_channels)\n",
    "            count += t_slice\n",
    "\n",
    "        local_chunk = local_chunk.T\n",
    "        self._close()\n",
    "\n",
    "        if nodes is not None:\n",
    "            if not np.all(nodes == np.arange(self.nb_channels)):\n",
    "                local_chunk = np.take(local_chunk, nodes, axis=1)\n",
    "\n",
    "        return self._scale_data_to_float32(local_chunk)\n",
    "\n",
    "    def read_chunk_adc(self, idx, chunk_size, padding=(0, 0), nodes=None):\n",
    "\n",
    "        t_start, t_stop = self._get_t_start_t_stop(idx, chunk_size, padding)\n",
    "        local_shape     = t_stop - t_start\n",
    "\n",
    "        local_chunk = np.zeros((self.nb_channels_adc, local_shape), dtype=self.data_dtype)\n",
    "        data_slice  = self._get_slice_adc_(t_start, t_stop)\n",
    "\n",
    "        self._open()\n",
    "        count = 0\n",
    "\n",
    "        for s in data_slice:\n",
    "            t_slice = len(s)//self.nb_channels_adc\n",
    "            local_chunk[:, count:count + t_slice] = self.data[s].reshape(self.nb_channels_adc, len(s)//self.nb_channels_adc)\n",
    "            count += t_slice\n",
    "\n",
    "        local_chunk = local_chunk.T\n",
    "        self._close()\n",
    "\n",
    "        if nodes is not None:\n",
    "            if not np.all(nodes == np.arange(self.nb_channels_adc)):\n",
    "                local_chunk = np.take(local_chunk, nodes, axis=1)\n",
    "\n",
    "        return self._scale_data_to_float32(local_chunk)\n",
    "\n",
    "    def write_chunk(self, time, data):\n",
    "\n",
    "        t_start     = time\n",
    "        t_stop      = time + data.shape[0]\n",
    "\n",
    "        if t_stop > self.duration:\n",
    "            t_stop  = self.duration\n",
    "\n",
    "        data = self._unscale_data_from_float32(data)\n",
    "        data_slice  = self._get_slice_(t_start, t_stop)\n",
    "\n",
    "        self._open(mode='r+')\n",
    "        count = 0\n",
    "        for s in data_slice:\n",
    "            t_slice      = len(s)//self.nb_channels\n",
    "            self.data[s] = data[count:count + t_slice, :].T.ravel()\n",
    "            count += t_slice\n",
    "\n",
    "        self._close()\n",
    "\n",
    "    def _open(self, mode='r'):\n",
    "        self.data = np.memmap(self.file_name, offset=self.data_offset, dtype=self.data_dtype, mode=mode)\n",
    "\n",
    "    def _close(self):\n",
    "        self.data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class H5File(DataFile):\n",
    "\n",
    "    description    = \"hdf5\"\n",
    "    extension      = [\".h5\", \".hdf5\"]\n",
    "    parallel_write = h5py.get_config().mpi\n",
    "    is_writable    = True\n",
    "\n",
    "    _required_fields = {'h5_key'        : str,\n",
    "                        'sampling_rate' : float}\n",
    "\n",
    "    _default_values  = {'dtype_offset'  : 'auto',\n",
    "                        'h5_key_adc'    : \"Data/Recording_0/AnalogStream/Stream_1/ChannelData\",\n",
    "                        'gain'          : 1.,\n",
    "                        'data_dtype'    : 'uint8',\n",
    "                        'nb_channels'   : 1}\n",
    "\n",
    "\n",
    "    def _check_compression(self):\n",
    "        # HDF5 does not support parallel writes with compression\n",
    "        if self.compression != '':\n",
    "            self.parallel_write = False\n",
    "            if self.is_master:\n",
    "                print_and_log(['Data are compressed thus parallel writing is disabled'], 'debug', logger)\n",
    "\n",
    "    def __check_valid_key__(self, key):\n",
    "        file       = h5py.File(self.file_name, mode='r')\n",
    "        all_fields = []\n",
    "        file.visit(all_fields.append)\n",
    "        if not key in all_fields:\n",
    "            print_and_log(['The key %s can not be found in the dataset! Keys found are:' %key,\n",
    "                         \", \".join(all_fields)], 'error', logger)\n",
    "            sys.exit(1)\n",
    "        file.close()\n",
    "\n",
    "    def _read_from_header(self):\n",
    "\n",
    "        self.__check_valid_key__(self.h5_key)\n",
    "        self._open()\n",
    "\n",
    "        header = {}\n",
    "        header['data_dtype']   = self.my_file.get(self.h5_key).dtype\n",
    "        self.compression       = self.my_file.get(self.h5_key).compression\n",
    "\n",
    "        self._check_compression()\n",
    "\n",
    "        self.size        = self.my_file.get(self.h5_key).shape\n",
    "\n",
    "        if self.size[0] > self.size[1]:\n",
    "            self.time_axis = 0\n",
    "            self._shape = (self.size[0], self.size[1])\n",
    "        else:\n",
    "            self.time_axis = 1\n",
    "            self._shape = (self.size[1], self.size[0])\n",
    "\n",
    "        header['nb_channels']  = self._shape[1]\n",
    "        self._close()\n",
    "\n",
    "        return header\n",
    "\n",
    "    def read_chunk(self, idx, chunk_size, padding=(0, 0), nodes=None):\n",
    "\n",
    "        t_start, t_stop = self._get_t_start_t_stop(idx, chunk_size, padding)\n",
    "\n",
    "        if nodes is None:\n",
    "            if self.time_axis == 0:\n",
    "                local_chunk = self.data[t_start:t_stop, :]\n",
    "            elif self.time_axis == 1:\n",
    "                local_chunk = self.data[:, t_start:t_stop].T\n",
    "        else:\n",
    "            if self.time_axis == 0:\n",
    "                local_chunk = self.data[t_start:t_stop, nodes]\n",
    "            elif self.time_axis == 1:\n",
    "                local_chunk = self.data[nodes, t_start:t_stop].T\n",
    "\n",
    "        return self._scale_data_to_float32(local_chunk)\n",
    "\n",
    "    def read_chunk_adc(self, idx, chunk_size, padding=(0, 0), nodes=None):\n",
    "\n",
    "        t_start, t_stop = self._get_t_start_t_stop(idx, chunk_size, padding)\n",
    "\n",
    "        if nodes is None:\n",
    "            local_chunk = self.data_adc[0,t_start:t_stop]\n",
    "        else:\n",
    "            local_chunk = self.data_adc[0,t_start:t_stop]\n",
    "\n",
    "        return self._scale_data_to_float32(local_chunk)\n",
    "\n",
    "    def write_chunk(self, time, data):\n",
    "\n",
    "        data = self._unscale_data_from_float32(data)\n",
    "\n",
    "        if self.time_axis == 0:\n",
    "            self.data[time:time+data.shape[0], :] = data\n",
    "        elif self.time_axis == 1:\n",
    "            self.data[:, time:time+data.shape[0]] = data.T\n",
    "\n",
    "    def _open(self, mode='r'):\n",
    "#        if mode in ['r+', 'w'] and self.parallel_write:\n",
    "#            self.my_file = h5py.File(self.file_name, mode=mode, driver='mpio', comm=comm)\n",
    "#        else:\n",
    "        self.my_file = h5py.File(self.file_name, mode=mode)\n",
    "\n",
    "        self.data = self.my_file.get(self.h5_key)\n",
    "        self.data_adc = self.my_file.get(self.h5_key_adc)\n",
    "\n",
    "    def _close(self):\n",
    "        self.my_file.close()\n",
    "        del self.data\n",
    "        del self.data_adc\n",
    "\n",
    "    @property\n",
    "    def h5_key(self):\n",
    "        return self.params['h5_key']\n",
    "\n",
    "    @property\n",
    "    def h5_key_adc(self):\n",
    "        return self.params['h5_key_adc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RawBinaryFile(DataFile):\n",
    "\n",
    "    description    = \"raw_binary\"\n",
    "    extension      = []\n",
    "    parallel_write = True\n",
    "    is_writable    = True\n",
    "\n",
    "    _required_fields = {'data_dtype'    : str,\n",
    "                        'sampling_rate' : float,\n",
    "                        'nb_channels'   : int}\n",
    "\n",
    "    _default_values  = {'dtype_offset'  : 'auto',\n",
    "                        'data_offset'   : 0,\n",
    "                        'gain'          : 1.}\n",
    "\n",
    "    def _read_from_header(self):\n",
    "        self._open()\n",
    "        self.size   = len(self.data)\n",
    "        self._shape = (self.size//self.nb_channels, int(self.nb_channels))\n",
    "        self._close()\n",
    "        return {}\n",
    "\n",
    "    def allocate(self, shape, data_dtype=None):\n",
    "        if data_dtype is None:\n",
    "            data_dtype = self.data_dtype\n",
    "\n",
    "        if self.is_master:\n",
    "            self.data = np.memmap(self.file_name, offset=self.data_offset, dtype=data_dtype, mode='w+', shape=shape)\n",
    "#        comm.Barrier()\n",
    "\n",
    "        self._read_from_header()\n",
    "        del self.data\n",
    "\n",
    "    def read_chunk(self, idx, chunk_size, padding=(0, 0), nodes=None):\n",
    "\n",
    "        t_start, t_stop = self._get_t_start_t_stop(idx, chunk_size, padding)\n",
    "        local_shape     = t_stop - t_start\n",
    "\n",
    "        self._open()\n",
    "        local_chunk  = self.data[t_start*self.nb_channels:t_stop*self.nb_channels]\n",
    "        local_chunk  = local_chunk.reshape(local_shape, self.nb_channels)\n",
    "        self._close()\n",
    "\n",
    "        if nodes is not None:\n",
    "            if not np.all(nodes == np.arange(self.nb_channels)):\n",
    "                local_chunk = np.take(local_chunk, nodes, axis=1)\n",
    "\n",
    "        return self._scale_data_to_float32(local_chunk)\n",
    "\n",
    "    def read_chunk_adc(self, idx, chunk_size, padding=(0, 0), nodes=None):\n",
    "            return self.read_chunk(idx, chunk_size, padding=padding, nodes=nodes)\n",
    "\n",
    "    def write_chunk(self, time, data):\n",
    "        self._open(mode='r+')\n",
    "\n",
    "        data = self._unscale_data_from_float32(data)\n",
    "        data = data.ravel()\n",
    "        self.data[self.nb_channels*time:self.nb_channels*time+len(data)] = data\n",
    "        self._close()\n",
    "\n",
    "\n",
    "    def _open(self, mode='r'):\n",
    "        self.data = np.memmap(self.file_name, offset=self.data_offset, dtype=self.data_dtype, mode=mode)\n",
    "\n",
    "    def _close(self):\n",
    "        self.data = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from numpy.lib.format import open_memmap\n",
    "\n",
    "class NumpyFile(RawBinaryFile):\n",
    "\n",
    "    description    = \"numpy\"\n",
    "    extension      = [\".npy\"]\n",
    "    parallel_write = True\n",
    "    is_writable    = True\n",
    "\n",
    "    _required_fields = {'sampling_rate' : float}\n",
    "\n",
    "    _default_values  = {'dtype_offset'  : 'auto',\n",
    "                        'gain'          : 1.}\n",
    "\n",
    "    def _read_from_header(self):\n",
    "        \n",
    "        header = {}\n",
    "\n",
    "        self._open()\n",
    "        self.size = self.data.shape\n",
    "\n",
    "        if self.size[0] > self.size[1]:\n",
    "            self.time_axis = 0\n",
    "            self._shape = (self.size[0], self.size[1])\n",
    "        else:\n",
    "            self.time_axis = 1\n",
    "            self._shape = (self.size[1], self.size[0])\n",
    "\n",
    "        header['nb_channels'] = self._shape[1]\n",
    "        header['data_dtype']  = self.data.dtype\n",
    "        self.size             = len(self.data)\n",
    "        self._close()\n",
    "\n",
    "        return header\n",
    "\n",
    "\n",
    "    def read_chunk(self, idx, chunk_size, padding=(0, 0), nodes=None):\n",
    "        \n",
    "        self._open()\n",
    "\n",
    "        t_start, t_stop = self._get_t_start_t_stop(idx, chunk_size, padding)\n",
    "\n",
    "        if self.time_axis == 0:\n",
    "            local_chunk  = self.data[t_start:t_stop, :].copy()\n",
    "        elif self.time_axis == 1:\n",
    "            local_chunk  = self.data[:, t_start:t_stop].copy().T\n",
    "        self._close()\n",
    "\n",
    "        if nodes is not None:\n",
    "            if not np.all(nodes == np.arange(self.nb_channels)):\n",
    "                local_chunk = np.take(local_chunk, nodes, axis=1)\n",
    "\n",
    "        return self._scale_data_to_float32(local_chunk)\n",
    "\n",
    "    def read_chunk_adc(self, idx, chunk_size, padding=(0, 0), nodes=None):\n",
    "        return self.read_chunk(idx, chunk_size, padding=padding, nodes=nodes)\n",
    "\n",
    "    def write_chunk(self, time, data):\n",
    "        self._open(mode='r+')\n",
    "        data = self._unscale_data_from_float32(data)\n",
    "        if self.time_axis == 0:\n",
    "            self.data[time:time+len(data)] = data\n",
    "        elif self.time_axis == 1:\n",
    "            self.data[:, time:time+len(data)] = data.T\n",
    "        self._close()\n",
    "\n",
    "\n",
    "    def _open(self, mode='r'):\n",
    "        self.data = open_memmap(self.file_name, mode=mode)\n",
    "\n",
    "\n",
    "    def _close(self):\n",
    "        self.data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_all_data(datafile:DataFile):\n",
    "    datafile.open()\n",
    "    if isinstance(datafile, RHDFile):\n",
    "        chunk_size = 1800960\n",
    "    else:\n",
    "        chunk_size =  datafile.duration\n",
    "    n_chunks, _ = datafile.analyze(chunk_size)\n",
    "    data = np.zeros((datafile.duration, datafile._shape[1]))\n",
    "    print(\"Loading the data... \"+str(round(0,2))+\"%    \",end='\\r',flush=True)\n",
    "    for idx in range(n_chunks):\n",
    "        data_tmp, t_offset = datafile.get_data(idx, chunk_size)\n",
    "        data[t_offset:t_offset+len(data_tmp)] = data_tmp\n",
    "        print(\"Loading the data... \"+str(round(100*(idx+1)/n_chunks,2))+\"%    \",end='\\r',flush=True)\n",
    "    print(\"Loading the data... \"+str(round(100,2))+\"%    \",end='\\r',flush=True)\n",
    "    return data\n",
    "    \n",
    "def load_all_data_adc(datafile:DataFile):\n",
    "    datafile.open()\n",
    "    if isinstance(datafile, RHDFile):\n",
    "        chunk_size = 1800960\n",
    "    else:\n",
    "        chunk_size =  datafile.duration\n",
    "    n_chunks, _ = datafile.analyze(chunk_size)\n",
    "    data = np.zeros(datafile.duration)\n",
    "    print(\"Loading the data... \"+str(round(0,2))+\"%    \",end='\\r',flush=True)\n",
    "    for idx in range(n_chunks):\n",
    "        data_tmp, t_offset = datafile.get_data_adc(idx, chunk_size)\n",
    "        if data_tmp.ndim == 2:\n",
    "            data_tmp = data_tmp[:,0]\n",
    "        data[t_offset:t_offset+len(data_tmp)] = data_tmp\n",
    "        print(\"Loading the data... \"+str(round(100*(idx+1)/n_chunks,2))+\"%    \",end='\\r',flush=True)\n",
    "    print(\"Loading the data... \"+str(round(100,2))+\"%    \",end='\\r',flush=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_utils.ipynb.\n",
      "Converted 02_processing.ipynb.\n",
      "Converted 03_modelling.ipynb.\n",
      "Converted 04_plotting.ipynb.\n",
      "Converted 05_database.ipynb.\n",
      "Converted 10_synchro.io.ipynb.\n",
      "Converted 11_synchro.extracting.ipynb.\n",
      "Converted 12_synchro.processing.ipynb.\n",
      "Converted 99_testdata.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
